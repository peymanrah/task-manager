# CoPilot Evaluation Framework — Spec

## Overview
**Repo:** https://github.com/peymanrah/CoPilotEvalFramework  
**Local:** `C:\Users\perahmat\OneDrive - Microsoft\Evaluation Framework`  
**Language:** Python  
**License:** MIT  
**Status:** POC

Automated UX evaluation framework for benchmarking **Microsoft Copilot** against **ChatGPT** and **Gemini** using stealth browser agents and LLM-as-judge evaluation.

## Architecture
```
User Feedback Data (Intent Taxonomy)
  → Synthetic Prompt Generator (synthetic_prompt_generator.py)
    → Parse Action/Object/Subject intents
    → Generate prompt variants (simple → expert)
    → Add context URLs
  → Stealth Browser Agent (chatbot_stealth_agent.py)
    → Copilot | ChatGPT | Gemini (Playwright + stealth)
    → Output: Full text + Screenshots per response
  → Evaluation Layer
    → Text Analysis (GPT-4 Judge)
    → Visual Analysis (GPT-4 Vision)
    → 7 UX Dimension Scoring
    → Competitive Leaderboards
```

## Evaluation Dimensions (7)
| Dimension | Weight | Description |
|-----------|--------|-------------|
| Factuality | 20% | Accuracy and grounding in source context |
| Helpfulness | 20% | Task completion and actionability |
| Safety | 15% | Absence of harmful/inappropriate content |
| Robustness | 10% | Consistency across prompt variations |
| Latency | 10% | Response time performance |
| Formatting | 15% | Visual structure and readability |
| Memory | 10% | Context retention (multi-turn) |

## Data Sources (Prompt Harvesting)
| Source | Description |
|--------|-------------|
| LMSYS Chatbot Arena | 33k+ battle-tested conversations (Gold Standard) |
| WildChat (AllenAI) | 1M+ real-world ChatGPT interactions, 68 languages |
| ShareGPT | User-shared conversations, long-context |
| OpenAssistant (OASST1) | Crowdsourced, human-annotated |
| Dolly | 15k high-quality Databricks prompts |
| Alpaca | 52k Stanford instruction-following data |

## Key Files
| File | Purpose |
|------|---------|
| `copilot_evaluation_framework.py` | Main framework: harvesting + categorization |
| `chatbot_stealth_agent.py` | Playwright stealth automation for 3 chatbots |
| `synthetic_prompt_generator.py` | Prompt generation from intent taxonomy |
| `chatbot_evaluation_webapp.py` | Web UI for evaluation |
| `llm_judge.py` / `agent_llm_judge.py` | GPT-4 based LLM-as-judge scoring |
| `evaluation_engine.py` | Core evaluation pipeline |
| `score_aggregator.py` | Multi-dimension score aggregation |
| `text_evaluator.py` | Text-based response analysis |
| `visual_feature_extractor.py` | Visual/screenshot analysis |
| `generate_winner_analysis.py` | Competitive analysis reports |
| `enhanced_prompt_harvester.py` | Enhanced HuggingFace harvester |
| `prompt_harvester_lite.py` | Lightweight harvester variant |
| `run_evaluation.py` | Evaluation CLI runner |
| `run_integrated_evaluation.py` | Full pipeline runner |

## Commands
```bash
# Setup
pip install -r requirements.txt
playwright install chromium

# Run stealth evaluation (N prompts)
python chatbot_stealth_agent.py 10

# Generate synthetic prompts
python run_generation.py

# Run LLM judge evaluation
python run_llm_judge.py

# Run full integrated evaluation
python run_integrated_evaluation.py

# Harvest prompts from HuggingFace
python copilot_evaluation_framework.py
python enhanced_prompt_harvester.py
```

## Output Structure
```
evaluation_results/
├── screenshots/
│   ├── SP_000001/
│   │   ├── copilot/
│   │   ├── chatgpt/
│   │   └── gemini/
│   └── ...
├── consolidated_results.csv
└── winner_analysis/
```

## Stealth Techniques
- Playwright-stealth for fingerprint masking
- Persistent browser profiles (`browser_profile/`)
- Human-like delays and interactions
- Realistic viewport and user agent

## Checklist
- [ ] Setup Python 3.9+ venv & install requirements
- [ ] Install Playwright + Chromium
- [ ] Configure OpenAI API key for GPT-4 judge
- [ ] Harvest prompts from HuggingFace sources
- [ ] Generate synthetic prompts from intent data
- [ ] Run stealth browser agent across all 3 chatbots
- [ ] Execute LLM-as-judge evaluation pipeline
- [ ] Aggregate scores and generate leaderboards
- [ ] Produce winner analysis & visualization reports
- [ ] Validate end-to-end pipeline

## Risks & Considerations
- **Bot detection:** Chatbot sites may update their detection; stealth config needs maintenance
- **API costs:** GPT-4 judge evaluation incurs OpenAI API costs per prompt
- **Rate limits:** Respect chatbot terms of service and rate limits
- **Data privacy:** No PII or proprietary data in the repo; CSVs are gitignored
- **Flakiness:** Browser automation can be flaky; retry logic needed
